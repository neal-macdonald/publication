{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f62df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.backend as K\n",
    "tf.__version__\n",
    "\n",
    "import os, sys\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import math\n",
    "import pathlib\n",
    "import skimage\n",
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8797a0cd-82d5-42fd-aaea-089c4f6c9fc2",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cdc1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHAPE = [476,476,3]\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 100\n",
    "# LEARNING_RATE=0.1\n",
    "# DECAY_RATE = LEARNING_RATE/EPOCHS\n",
    "\n",
    "RANDOM_SEED = 101\n",
    "random.seed(RANDOM_SEED)\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "num_classes = 18\n",
    "\n",
    "clustering_folder = r\"\" # Folder where unsupervised clustering results are stored\n",
    "weights_folder = r\"\" # Folder where model weights are saved\n",
    "test_folder = r\"\" # Path to folder where test data is stored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef9d353-1e5a-416f-8ada-88be5be255d5",
   "metadata": {},
   "source": [
    "### Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62846cea-2ff0-4814-9e82-90d361e3431d",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_dict = {}\n",
    "total_count = len([os.path.join(clustering_folder,str(item)) for item in list(pathlib.Path(clustering_folder).glob('*/*.jpg')) if str(item).endswith('.jpg')])\n",
    "for folder in os.listdir(clustering_folder):\n",
    "    classf = folder.split('_')[-1]\n",
    "    countf = len([file for file in os.listdir(os.path.join(clustering_folder,folder))])\n",
    "    weight_dict[int(classf)] = math.log(countf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f228aed0-5325-467a-8939-56a4b4eb8e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_images(image,label):\n",
    "    image = tf.image.per_image_standardization(image)\n",
    "    image = tf.math.divide(tf.math.add(image,tf.math.abs(tf.math.reduce_min(image))),tf.math.add(tf.math.reduce_max(image),tf.math.abs(tf.math.reduce_min(image)))+0.00001)\n",
    "    return image,label\n",
    "\n",
    "train_ds_series = tf.keras.utils.image_dataset_from_directory(\n",
    "    clustering_folder,\n",
    "    labels=\"inferred\",\n",
    "    label_mode = 'categorical',\n",
    "    class_names = [f'class_{i}' for i in range(len(os.listdir(clustering_folder)))],\n",
    "    shuffle=True,\n",
    "    seed=RANDOM_SEED,\n",
    "    validation_split=0.05,\n",
    "    subset='training',\n",
    "    image_size=(SHAPE[0],SHAPE[1]),\n",
    "    batch_size=None).map(normalize_images,num_parallel_calls=AUTOTUNE).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "\n",
    "val_ds_series = tf.keras.utils.image_dataset_from_directory(\n",
    "    clustering_folder,\n",
    "    labels=\"inferred\",\n",
    "    label_mode = 'categorical',\n",
    "    class_names = [f'class_{i}' for i in range(len(os.listdir(clustering_folder)))],\n",
    "    shuffle=True,\n",
    "    seed=RANDOM_SEED,\n",
    "    validation_split=0.05,\n",
    "    subset='validation',\n",
    "    image_size=(SHAPE[0],SHAPE[1]),\n",
    "    batch_size=None).map(normalize_images,num_parallel_calls=AUTOTUNE).batch(BATCH_SIZE).prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57693a0-84ba-48aa-b4a6-7b8e67619969",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20))\n",
    "for i,image in enumerate(train_ds_series.take(25)):\n",
    "    plt.subplot(5, 5, i + 1)\n",
    "    plt.imshow(image[0][0,:,:,:])\n",
    "    plt.title(f\"Class {tf.argmax(image[1][0])}\")\n",
    "    plt.axis(\"off\")\n",
    "plt.subplots_adjust(hspace=0.1, wspace=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7f2574-527b-4b95-a83c-4d64bfd119f9",
   "metadata": {},
   "source": [
    "### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7a59cb-a41d-4323-8fc9-6cca8d0d14ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_resnet_classifier():\n",
    "    inputs = keras.Input(shape=(SHAPE[0],SHAPE[1],3))\n",
    "    # x = keras.layers.RandomCrop(128,128)(inputs)\n",
    "    num_filters = 32\n",
    "    \n",
    "    def relu_batchn(inputs: tf.Tensor) -> tf.Tensor:\n",
    "        relu = keras.layers.ReLU()(inputs)\n",
    "        batchn = keras.layers.BatchNormalization()(relu)\n",
    "        return batchn\n",
    "    \n",
    "    def res_block(x: tf.Tensor, downsample: bool, filters: int, kernel_size: int = 3) -> tf.Tensor:\n",
    "        y = keras.layers.Conv2D(kernel_size=kernel_size, strides=(1 if not downsample else 2), filters=filters,padding=\"same\")(x)\n",
    "        y = relu_batchn(y)\n",
    "        y = keras.layers.Conv2D(kernel_size=(5 if i==0 else 3),strides=1,filters=filters,padding=\"same\")(y)\n",
    "        if downsample:\n",
    "            x = keras.layers.Conv2D(kernel_size=1,strides=2,filters=filters,padding=\"same\")(x)\n",
    "        out = keras.layers.Add()([x, y])\n",
    "        out = relu_batchn(out)\n",
    "        return out\n",
    "    \n",
    "    x = keras.layers.Conv2D(num_filters, kernel_size = 5, strides = 1, padding=\"same\")(inputs)\n",
    "    x = relu_batchn(x)\n",
    "    \n",
    "    blocks_list = [2,3,3,3,3,2]\n",
    "    for i in range(len(blocks_list)):\n",
    "        num_blocks = blocks_list[i]\n",
    "        for j in range(num_blocks):\n",
    "            x = res_block(x, downsample=(j==0 and i!=0), filters=num_filters)\n",
    "        num_filters += 32\n",
    "\n",
    "    x = keras.layers.Conv2D(num_filters+32*2, kernel_size = 3, strides = 2, padding=\"same\")(x)\n",
    "    x = relu_batchn(x)\n",
    "    x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = keras.layers.Dropout(0.3)(x)\n",
    "    output = keras.layers.Dense(num_classes,kernel_regularizer=tf.keras.regularizers.l2(0.0001))(x)\n",
    "    \n",
    "    model = keras.models.Model(inputs, output, name='Classifier')\n",
    "    return model\n",
    "\n",
    "def create_cnn_classifier():\n",
    "    inputs = keras.Input(shape=(SHAPE[0],SHAPE[1],3))\n",
    "    x = keras.layers.Conv2D(64, 5, padding='same',strides=(1,1))(inputs)\n",
    "    \n",
    "    for filt in [128,128,128,128,128,128]:\n",
    "        x = keras.layers.Conv2D(filt, 5, strides=2, padding='same')(x)\n",
    "        x = keras.layers.BatchNormalization()(x)\n",
    "        x = keras.layers.ReLU()(x)\n",
    "        x = keras.layers.Conv2D(filt, 5, padding='same')(x)\n",
    "        x = keras.layers.BatchNormalization()(x)\n",
    "        x = keras.layers.ReLU()(x)\n",
    "    x = keras.layers.Conv2D(256, 3,strides=2, padding='same')(x)\n",
    "    x = keras.layers.BatchNormalization()(x) \n",
    "    x = keras.layers.ReLU()(x)   \n",
    "    x = keras.layers.GlobalMaxPooling2D()(x)   \n",
    "    output = keras.layers.Dense(num_classes,kernel_regularizer=tf.keras.regularizers.l2(0.0001))(x)\n",
    "    \n",
    "    model = keras.models.Model(inputs, output, name='Classifier')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11d9cd5-e737-49fc-ac33-09aed1c362aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_resnet_classifier()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eed2200-3547-4567-b424-bc3555674121",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.005, momentum=0.9), \n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.1),\n",
    "    metrics=['accuracy',tf.keras.metrics.TopKCategoricalAccuracy(k=3)])\n",
    "\n",
    "model_early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_top_k_categorical_accuracy', patience=15, min_delta = 0.01, mode='max', restore_best_weights=True)\n",
    "history = model.fit(train_ds_series, validation_data=val_ds_series, shuffle=True, epochs=EPOCHS, class_weight=weight_dict, callbacks=[model_early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c6f57b-21c0-48c8-b493-9b3dbf10b9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(30):\n",
    "    hit = next(iter(train_ds_series))\n",
    "    arr = model(np.expand_dims(hit[0][0],0),training=False).numpy().flatten()\n",
    "    print(np.argpartition(arr,-3)[-3:],\n",
    "        np.argmax(hit[1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d7b2c5-7803-4de1-ae40-1ed59bf1380e",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_dir = os.path.join(weights_folder,'supervised')\n",
    "if not os.path.isdir(weights_dir):\n",
    "    os.makedirs(weights_dir, exist_ok=True)\n",
    "model.save(os.path.join(weights_dir,\"supervised_weights.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0649d4-393d-471a-803f-75d1812e33c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.models.load_model(os.path.join(weights_dir,\"supervised_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7f5859-0b59-48d7-8b2c-baa5073edfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(train_ds_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e0241f-c05e-4a00-a618-edd16b8fd847",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [os.path.join(path,name) for path, s, files in os.walk(test_folder) for name in files if name.endswith('jpg')]\n",
    "unknown_path = test[0]\n",
    "# img = np.asarray(Image.open(img))[:478,:478,:]\n",
    "    \n",
    "def normalize_image_only(image):\n",
    "    image = tf.convert_to_tensor(image)\n",
    "    image = tf.keras.layers.Resizing(SHAPE[0],SHAPE[1])(image)\n",
    "    image = tf.image.per_image_standardization(image)\n",
    "    image = tf.math.divide(tf.math.add(image,tf.math.abs(tf.math.reduce_min(image))),tf.math.add(tf.math.reduce_max(image),tf.math.abs(tf.math.reduce_min(image)))+0.00001)\n",
    "    return image\n",
    "\n",
    "img = normalize_image_only(np.asarray(Image.open(unknown_path)))\n",
    "\n",
    "arr = model(np.expand_dims(img,0),training=False).numpy().flatten()\n",
    "class_matches = np.argpartition(arr,-3)\n",
    "class_matches = class_matches[::-1]\n",
    "filtered_matches = {key:arr[key] for key in class_matches if arr[key]>2}\n",
    "print(filtered_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95e8ebc-464c-4317-8417-2ea3a07204a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "length = val_ds_series.cardinality().numpy()\n",
    "top1 = length\n",
    "top2 = length\n",
    "top3 = length\n",
    "for input_data in tqdm(val_ds_series):\n",
    "    prediction = np.argpartition(model.predict(np.expand_dims(input_data[0],0),verbose=0)[0],-3)\n",
    "    label = np.argmax(input_data[1])\n",
    "    # print(prediction[-3:],prediction[-2:],prediction[-1:],label)\n",
    "    if label not in prediction[-1:]:\n",
    "        top1-=1\n",
    "    if label not in prediction[-2:]:\n",
    "        top2-=1\n",
    "    if label not in prediction[-3:]:\n",
    "        top3-=1\n",
    "print(top1,top2,top3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9a3161-7383-45f6-a5fb-799f2b2965fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(top1/length,top2/length,top3/length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f72920-c35e-4416-bec4-5a054208b804",
   "metadata": {},
   "source": [
    "#### Process Test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368571aa-7dbf-4434-ada7-f7ba9bb417c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_unknown(unknown_path,model_path,class_name=None,save=True):\n",
    "    \n",
    "    def normalize_image_only(image):\n",
    "        image = tf.convert_to_tensor(image)\n",
    "        image = tf.keras.layers.Resizing(SHAPE[0],SHAPE[1])(image)\n",
    "        image = tf.image.per_image_standardization(image)\n",
    "        image = tf.math.divide(tf.math.add(image,tf.math.abs(tf.math.reduce_min(image))),tf.math.add(tf.math.reduce_max(image),tf.math.abs(tf.math.reduce_min(image)))+0.00001)\n",
    "        return image\n",
    "    \n",
    "    img = normalize_image_only(np.asarray(Image.open(unknown_path)))\n",
    "    \n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    \n",
    "    arr = model(np.expand_dims(img,0),training=False).numpy().flatten()\n",
    "    class_matches = np.argpartition(arr,-3)[-3:]\n",
    "    class_matches = class_matches[::-1]\n",
    "\n",
    "    fig, ax = plt.subplots(4,4,figsize=(10,10))\n",
    "    ax[0,0].imshow(img)\n",
    "    if class_name != None:\n",
    "        ax[0,0].set_title(class_name)\n",
    "    else:\n",
    "        ax[0,0].set_title('Unknown Image')\n",
    "    ax[0,0].axis('off')\n",
    "    for i in [1,2,3]:\n",
    "        ax[0,i].axis('off')\n",
    "\n",
    "    for r in [1,2,3]:\n",
    "        classx = [file for file in os.listdir(os.path.join(clustering_folder,f'class_{class_matches[r-1]}'))]\n",
    "        random.shuffle(classx)\n",
    "        for c in range(4):\n",
    "            c_idx = random.randint(0,25)\n",
    "            ax[r,c].set_title(f'Class {str(class_matches[r-1])}')\n",
    "            class_data = Image.open(os.path.join(clustering_folder,f'class_{class_matches[r-1]}',classx[c_idx]))\n",
    "            ax[r,c].imshow(normalize_image_only(np.asarray(class_data)))\n",
    "            ax[r,c].axis('off')\n",
    "    plt.tight_layout()\n",
    "    if save is True:\n",
    "        out_split = os.path.split(unknown_path)\n",
    "        out_path = os.path.join(os.path.split(out_split[0])[0],f'evaluated_{out_split[1]}')\n",
    "        plt.savefig(out_path)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704d71f9-8383-461a-81de-daa5005a7bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in test:\n",
    "    print(file)\n",
    "    process_unknown(unknown_path = file,\n",
    "                      model_path = os.path.join(weights_dir,\"supervised_weights.h5\"),\n",
    "                      class_name = os.path.split(os.path.split(file)[0])[-1])"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m86",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m86"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
